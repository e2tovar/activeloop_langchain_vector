{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.environ.get('OPENAI_API_KEY')\n",
    "os.environ['ACTIVELOOP_TOKEN'] = os.environ.get('ACTIVELOOP_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to the query 'A cat is sitting on a mat.':\n",
      "The cat is on the mat.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "# Perform a similarity search for a given query\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Document 1\n",
      "Embedding: [0.03201056271791458, 0.038009535521268845, 0.008137303404510021, -0.05043879523873329, -0.05071724206209183]\n",
      "Text: Document 2\n",
      "Embedding: [0.03084605187177658, 0.061166975647211075, 0.0210378747433424, -0.04848083481192589, -0.0537794791162014]\n",
      "Text: Document 3\n",
      "Embedding: [0.04238653555512428, 0.008868656121194363, 0.023097645491361618, -0.049794841557741165, -0.035510122776031494]\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings\n",
    "for text, embedding in zip(documents, doc_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding with cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello from Cohere!\n",
      "Embedding: [0.23461914, 0.50146484, -0.048828125, 0.13989258, -0.18029785]\n",
      "Text: مرحبًا من كوهير!\n",
      "Embedding: [0.25317383, 0.30004883, 0.0104904175, 0.12573242, -0.18273926]\n",
      "Text: Hallo von Cohere!\n",
      "Embedding: [0.10266113, 0.28320312, -0.050201416, 0.23706055, -0.07159424]\n",
      "Text: Bonjour de Cohere!\n",
      "Embedding: [0.15185547, 0.28173828, -0.057281494, 0.11743164, -0.04385376]\n",
      "Text: ¡Hola desde Cohere!\n",
      "Embedding: [0.25146484, 0.43139648, -0.0859375, 0.24682617, -0.11706543]\n",
      "Text: Olá do Cohere!\n",
      "Embedding: [0.18664551, 0.39038086, -0.045898438, 0.14562988, -0.11254883]\n",
      "Text: Ciao da Cohere!\n",
      "Embedding: [0.115722656, 0.43310547, -0.026168823, 0.14575195, 0.07080078]\n",
      "Text: 您好，来自 Cohere！\n",
      "Embedding: [0.24609375, 0.30859375, -0.111694336, 0.26635742, -0.051086426]\n",
      "Text: कोहेरे से नमस्ते!\n",
      "Embedding: [0.1932373, 0.6352539, 0.03213501, 0.117370605, -0.26098633]\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "# Initialize the CohereEmbeddings object\n",
    "cohere = CohereEmbeddings(\n",
    "\tmodel=\"embed-multilingual-v2.0\",\n",
    "\tcohere_api_key=\"lmAnD5JRWeE7F2RI2CmW2sQrTqp3PhNtypCLKFyi\"\n",
    ")\n",
    "\n",
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\", \n",
    "    \"مرحبًا من كوهير!\", \n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\", \n",
    "    \"¡Hola desde Cohere!\", \n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\", \n",
    "    \"您好，来自 Cohere！\", \n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing in Deep Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some documents using the RecursiveCharacterTextSplitter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Napoleon Bonaparte was born in 15 August 1769\n",
      "Embedding: [0.23461914, 0.50146484, -0.048828125, 0.13989258, -0.18029785]\n",
      "------------------------------------------------\n",
      "Text: Louis XIV was born in 5 September 1638\n",
      "Embedding: [0.25317383, 0.30004883, 0.0104904175, 0.12573242, -0.18273926]\n",
      "------------------------------------------------\n",
      "Text: Lady Gaga was born in 28 March 1986\n",
      "Embedding: [0.10266113, 0.28320312, -0.050201416, 0.23706055, -0.07159424]\n",
      "------------------------------------------------\n",
      "Text: Michael Jeffrey Jordan was born in 17 February 1963\n",
      "Embedding: [0.15185547, 0.28173828, -0.057281494, 0.11743164, -0.04385376]\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions of each embedding\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a Deep Lake database and load our documents into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 4 embeddings in 1 batches of size 4:: 100%|██████████| 1/1 [00:08<00:00,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://e2tovar/langchain_course_embeddings3', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      "   text       text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cbc343c9-d5bc-11ee-b341-482ae342e600',\n",
       " 'cbc343ca-d5bc-11ee-b4d0-482ae342e600',\n",
       " 'cbc343cb-d5bc-11ee-b05e-482ae342e600',\n",
       " 'cbc343cc-d5bc-11ee-9fd9-482ae342e600']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"e2tovar\"\n",
    "my_activeloop_dataset_name = \"langchain_course_embeddings3\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a RetrievalQA chain in LangChain and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'When was Michael Jordan born?',\n",
       " 'result': 'Michael Jordan was born on 17 February 1963.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# istantiate the llm wrapper\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "# create the question-answering chain\n",
    "qa_chain = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# ask a question to the chain\n",
    "qa_chain.invoke(\"When was Michael Jordan born?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down each step to understand how these technologies work together.\n",
    "\n",
    "1-OpenAI and LangChain Integration: LangChain, a library built for chaining NLP models, is designed to work seamlessly with OpenAI's GPT-3.5-turbo model for language understanding and generation. You've initialized OpenAI embeddings using OpenAIEmbeddings(), and these embeddings are later used to transform the text into a high-dimensional vector representation. This vector representation captures the semantic essence of the text and is essential for information retrieval tasks.\n",
    "\n",
    "2-Deep Lake: Deep Lake is a Vector Store for creating, storing, and querying vector representations (also known as embeddings) of data.\n",
    "\n",
    "3-Text Retrieval: Using the db.as_retriever() function, you've transformed the Deep Lake dataset into a retriever object. This object is designed to fetch the most relevant pieces of text from the dataset based on the semantic similarity of their embeddings.\n",
    "\n",
    "4-Question Answering: The final step involves setting up a RetrievalQA chain from LangChain. This chain is designed to accept a natural language question, transform it into an embedding, retrieve the most relevant document chunks from the Deep Lake dataset, and generate a natural language answer. The ChatOpenAI model, which is the underlying model of this chain, is responsible for both the question embedding and the answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
