{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ucL9y4VoUJui"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "os.environ['OPENAI_API_KEY'] = os.environ.get('OPENAI_API_KEY')\n",
        "os.environ['ACTIVELOOP_TOKEN'] = os.environ.get('ACTIVELOOP_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-J9LsngZsfp"
      },
      "source": [
        "# Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44NeYe0GXe32"
      },
      "source": [
        "## Invoke"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQn4558HUPvI",
        "outputId": "35edbf54-a44e-41d7-806f-2d2e0f374970"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'word': 'artificial', 'text': '\\n\\nSynthetic'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "prompt_template = \"What is a word to replace the following: {word}?\"\n",
        "\n",
        "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate.from_template(prompt_template)\n",
        ")\n",
        "llm_chain.invoke(\"artificial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lGTqvJxXjMZ"
      },
      "source": [
        "## Apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhrI8CggVtJo",
        "outputId": "f2673059-65b2-4ba4-df2c-063688277c88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': '\\n\\nSynthetic'},\n",
              " {'text': '\\n\\nCleverness'},\n",
              " {'text': '\\n\\nandroid'}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_list = [\n",
        "    {\"word\": \"artificial\"},\n",
        "    {\"word\": \"intelligence\"},\n",
        "    {\"word\": \"robot\"}\n",
        "]\n",
        "\n",
        "llm_chain.apply(input_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5jB5LBJXk9s"
      },
      "source": [
        "## Generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The .generate() method will return an instance of LLMResult, which provides more information. For example, the finish_reason key indicates the reason behind the stop of the generation process. It could be stopped, meaning the model decided to finish or reach the length limit. There is other self-explanatory information like the number of total used tokens or the used model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYi0o5KqV68n",
        "outputId": "e656827f-2299-423e-f4a8-af09a1402fae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\nSynthetic', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nCleverness', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nandroid', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 33, 'total_tokens': 42}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('ff1b7c75-53c4-4179-9d61-9a1230b376c2')), RunInfo(run_id=UUID('327d6a53-97d7-42da-a88a-9694e7e38e1d')), RunInfo(run_id=UUID('c09c2317-37db-4bf2-841a-c328eb58aec9'))])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.generate(input_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4ej9ZXXm-E"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next method we will discuss is .predict(). (which could be used interchangeably with .run()) Its best use case is to pass multiple inputs for a single prompt. However, it is possible to use it with one input variable as well. The following prompt will pass both the word we want a substitute for and the context the model must consider."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q1BvtRiXuLg"
      },
      "source": [
        "#### Multiple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OX4RrRrlXvWm",
        "outputId": "c9cc9d8a-1585-4f6d-ca5f-e752c04c1fd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nVentilation system'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_template = \"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
        "\n",
        "llm_chain.predict(word=\"fan\", context=\"house\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NpdjaLWwYQQ1",
        "outputId": "5ebe6455-95a2-4167-e235-d49982f9fe0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nSupporter'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict(word=\"fan\", context=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9P5gGwxClJeL",
        "outputId": "560ea2b2-b540-42a8-c18a-19fcca6f0e70"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nVentilator'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llm_chain.run(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNPOT6iAbt1l"
      },
      "source": [
        "### from string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h6T5_9k2bx_N"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\"\"\n",
        "llm_chain = LLMChain.from_string(llm=llm, template=template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AkE6wx8Vb9Ns",
        "outputId": "dbedb888-49d7-43da-88a5-05472fbea85d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nSupporter'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chain.predict(word=\"fan\", context=\"object\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRIaIXSKZu6U"
      },
      "source": [
        "# Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As discussed, the output parsers can define a data schema to generate correctly formatted responses. It wouldnâ€™t be an end-to-end pipeline without using parsers to extract information from the LLM textual output. The following example shows the use of CommaSeparatedListOutputParser class with the PromptTemplate to ensure the results will be in a list format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aIEZWDQtZwKw",
        "outputId": "aa45ef77-7ea6-42ea-c613-94ac0accfb88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['synthetic',\n",
              " 'man-made',\n",
              " 'fake',\n",
              " 'imitation',\n",
              " 'simulated',\n",
              " 'faux',\n",
              " 'fabricated',\n",
              " 'counterfeit',\n",
              " 'ersatz',\n",
              " 'false',\n",
              " 'pretend',\n",
              " 'mock',\n",
              " 'spurious',\n",
              " 'bogus',\n",
              " 'phony',\n",
              " 'contrived',\n",
              " 'manufactured',\n",
              " 'unnatural',\n",
              " 'inauthentic']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[]),\n",
        "    output_parser=output_parser)\n",
        "\n",
        "llm_chain.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b8_oGlAcx3F"
      },
      "source": [
        "# Conversational Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on the application, memory is the next component that will complete a chain. LangChain provides a ConversationalChain to track previous prompts and responses using the ConversationalBufferMemory class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTYpcUktae6w",
        "outputId": "3c7c83dd-5ba6-44f6-bbe7-baf153c97232"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Synthetic',\n",
              " 'simulated',\n",
              " 'man-made',\n",
              " 'fake',\n",
              " 'imitation',\n",
              " 'fabricated',\n",
              " 'manufactured',\n",
              " 'engineered',\n",
              " 'virtual',\n",
              " 'computer-generated',\n",
              " 'digital',\n",
              " 'non-natural',\n",
              " 'inauthentic',\n",
              " 'contrived',\n",
              " 'ersatz',\n",
              " 'pseudo',\n",
              " 'phony',\n",
              " 'false',\n",
              " 'unreal',\n",
              " 'pretend',\n",
              " 'counterfeit',\n",
              " 'spurious',\n",
              " 'bogus',\n",
              " 'sham',\n",
              " 'feigned',\n",
              " 'deceptive',\n",
              " 'fraudulent',\n",
              " 'specious',\n",
              " 'fallacious',\n",
              " 'misleading',\n",
              " 'delusive',\n",
              " 'illusory',\n",
              " 'fictitious',\n",
              " 'make-believe',\n",
              " 'imaginary',\n",
              " 'unreal',\n",
              " 'fictive',\n",
              " 'fanciful',\n",
              " 'fantastic',\n",
              " 'mythical',\n",
              " 'legendary',\n",
              " 'mythological',\n",
              " 'fabricated',\n",
              " 'created',\n",
              " 'contrived',\n",
              " 'invented',\n",
              " 'made-up',\n",
              " 'unreal',\n",
              " 'imaginary',\n",
              " 'fictional',\n",
              " 'mythical',\n",
              " 'legendary',\n",
              " 'mythological',\n",
              " 'fanciful',\n",
              " 'fantastic',\n",
              " 'make-believe',\n",
              " 'pretend',\n",
              " 'simulated',\n",
              " 'virtual',\n",
              " 'computer-generated',\n",
              " 'digital',\n",
              " 'non-natural',\n",
              " 'inauthentic',\n",
              " 'ersatz',\n",
              " 'pseudo',\n",
              " 'phony',\n",
              " 'false',\n",
              " 'counterfeit',\n",
              " 'spurious',\n",
              " 'bogus',\n",
              " 'sham',\n",
              " 'feigned',\n",
              " 'deceptive',\n",
              " 'fraudulent',\n",
              " 'specious',\n",
              " 'fallacious',\n",
              " 'misleading',\n",
              " 'delusive',\n",
              " 'illusory',\n",
              " 'fictitious',\n",
              " 'make-believe',\n",
              " 'imaginary',\n",
              " 'unreal',\n",
              " 'fictive',\n",
              " 'fanciful',\n",
              " 'fantastic',\n",
              " 'mythical',\n",
              " 'legendary',\n",
              " 'mythological',\n",
              " 'fabricated',\n",
              " 'created',\n",
              " 'contrived',\n",
              " 'invented',\n",
              " 'made-up',\n",
              " 'unreal',\n",
              " 'imaginary',\n",
              " 'fictional',\n",
              " 'mythical',\n",
              " 'legendary',\n",
              " 'mythological',\n",
              " 'fanciful',\n",
              " 'fantastic,']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory(),\n",
        "    output_parser=output_parser\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"List all possible words as substitute for 'artificial' as comma separated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7_vgaBgeWWG",
        "outputId": "0e9c50d3-a48f-4083-bafe-4944a53dda26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"['make-believe'\", \"'pretend'\", \"'simulated'\", \"'virtual']\"]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"And the next 4?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kz12V6FhjC3"
      },
      "source": [
        "# Debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGRoPJ1xhtTE",
        "outputId": "4bfd9840-47a9-4967-8c9d-e6a9ff0fbaa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mList all possible words as substitute for 'artificial' as comma separated.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n1. Synthetic\\n2. Man-made\\n3. Faux\\n4. Imitation\\n5. Simulated\\n6. Fake\\n7. Replicated\\n8. Manufactured\\n9. Counterfeit\\n10. Mock\\n11. Pretend\\n12. False\\n13. Inauthentic\\n14. Unnatural\\n15. Fabricated'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
        "\n",
        "Current conversation:\n",
        "{history}\n",
        "\n",
        "{input}\"\"\"\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
        "    memory=ConversationBufferMemory(),\n",
        "    verbose=True)\n",
        "\n",
        "conversation.predict(input=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XI9e40ui1yX"
      },
      "source": [
        "# Sequential Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another helpful feature is using a sequential chain that concatenates multiple chains into one. The following code shows a sample usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "A16wajt2hxLE"
      },
      "outputs": [],
      "source": [
        "# poet\n",
        "poet_template: str = \"\"\"You are an American poet, your job is to come up with\\\n",
        "poems based on a given theme.\n",
        "\n",
        "Here is the theme you have been asked to generate a poem on:\n",
        "{input}\\\n",
        "\"\"\"\n",
        "\n",
        "poet_prompt_template: PromptTemplate = PromptTemplate(\n",
        "    input_variables=[\"input\"], template=poet_template)\n",
        "\n",
        "# creating the poet chain\n",
        "poet_chain: LLMChain = LLMChain(\n",
        "    llm=llm, output_key=\"poem\", prompt=poet_prompt_template)\n",
        "\n",
        "# critic\n",
        "critic_template: str = \"\"\"You are a critic of poems, you are tasked\\\n",
        "to inspect the themes of poems. Identify whether the poem includes romantic expressions or descriptions of nature.\n",
        "\n",
        "Your response should be in the following format, as a Python Dictionary.\n",
        "poem: this should be the poem you received\n",
        "Romantic_expressions: True or False\n",
        "Nature_descriptions: True or False\n",
        "\n",
        "Here is the poem submitted to you:\n",
        "{poem}\\\n",
        "\"\"\"\n",
        "\n",
        "critic_prompt_template: PromptTemplate = PromptTemplate(\n",
        "    input_variables=[\"poem\"], template=critic_template)\n",
        "\n",
        "# creating the critic chain\n",
        "critic_chain: LLMChain = LLMChain(\n",
        "    llm=llm, output_key=\"critic_verified\", prompt=critic_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "overall_chain = SimpleSequentialChain(chains=[poet_chain, critic_chain])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input': 'the beauty of nature', 'output': \"\\n\\npoem: Nature's Beauty\\nRomantic_expressions: False\\nNature_descriptions: True\"}\n"
          ]
        }
      ],
      "source": [
        "# Run the poet and critic chain with a specific theme\n",
        "theme: str = \"the beauty of nature\"\n",
        "review = overall_chain.invoke(theme)\n",
        "\n",
        "# Print the review to see the critic's evaluation\n",
        "print(review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs4Chc0iKaj3"
      },
      "source": [
        "# Custom Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3tCjI4DtKbTG"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "class ConcatenateChain(Chain):\n",
        "    chain_1: LLMChain\n",
        "    chain_2: LLMChain\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        # Union of the input keys of the two chains.\n",
        "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
        "        return list(all_input_vars)\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return ['concat_output']\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
        "        output_1 = self.chain_1.run(inputs)\n",
        "        output_2 = self.chain_2.run(inputs)\n",
        "        return {'concat_output': output_1 + output_2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-W3ZqALLbwP",
        "outputId": "83f65c01-5573-403f-9180-7a2b60a41b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated output:\n",
            "\n",
            "\n",
            "Artificial means something that is made or produced by humans, rather than occurring naturally. It can also refer to something that is not genuine or authentic, but rather created or imitated to appear real.\n",
            "\n",
            "Synthetic\n"
          ]
        }
      ],
      "source": [
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"word\"],\n",
        "    template=\"What is the meaning of the following word '{word}'?\",\n",
        ")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"word\"],\n",
        "    template=\"What is a word to replace the following: {word}?\",\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"artificial\")\n",
        "print(f\"Concatenated output:\\n{concat_output}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
