{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsV8LDNckFNY",
        "outputId": "04814f16-7f2a-4134-8bdf-e151a43e466f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfBouz-GkHpo",
        "outputId": "de8f21da-8a18-4281-ca4e-d77089127cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the main advantage of quantum computing over classical computing?\n",
            "Answer: The main advantage of quantum computing over classical computing is the ability to solve complex problems faster.\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0)\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided, answer\n",
        "with \"I don't know\".\n",
        "Context: Quantum computing is an emerging field that leverages quantum mechanics to solve complex problems faster than classical computers.\n",
        "...\n",
        "Question: {query}\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Set the query you want to ask\n",
        "input_data = {\"query\": \"What is the main advantage of quantum computing over classical computing?\"}\n",
        "\n",
        "# Run the LLMChain to get the AI-generated answer\n",
        "response = chain.invoke(input_data)\n",
        "\n",
        "print(\"Question:\", input_data[\"query\"])\n",
        "print(\"Answer:\", response['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more advanced usage, you can create a FewShotPromptTemplate with an ExampleSelector to select a subset of examples that will be most informative for the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4nUPXHskR39",
        "outputId": "8ec568ca-0ad5-490e-c98a-2837c562a871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " tropical forests and mangrove swamps\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\"animal\": \"lion\", \"habitat\": \"savanna\"},\n",
        "    {\"animal\": \"polar bear\", \"habitat\": \"Arctic ice\"},\n",
        "    {\"animal\": \"elephant\", \"habitat\": \"African grasslands\"}\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "Animal: {animal}\n",
        "Habitat: {habitat}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"animal\", \"habitat\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Identify the habitat of the given animal\",\n",
        "    suffix=\"Animal: {input}\\nHabitat:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the dynamic_prompt\n",
        "chain = LLMChain(llm=llm, prompt=dynamic_prompt)\n",
        "\n",
        "# Run the LLMChain with input_data\n",
        "input_data = {\"input\": \"tiger\"}\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4djEqXz2kuVw"
      },
      "outputs": [],
      "source": [
        "prompt_template.save(\"awesome_prompt.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDEcazXml-W_"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import load_prompt\n",
        "loaded_prompt = load_prompt(\"awesome_prompt.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's explore more examples using different types of Prompt Templates. In the next example, we see how to use a few shot prompts to teach the LLM by providing examples to respond sarcastically to questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0ufnnNyl_6q",
        "outputId": "0234aff7-a038-49bf-a58e-1cb1a66ebbf9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "c:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Invent a time machine and go back to the 1950s when quantum computing was just a twinkle in a physicist's eye.\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How do I become a better programmer?\",\n",
        "        \"answer\": \"Try talking to a rubber duck; it works wonders.\"\n",
        "    }, {\n",
        "        \"query\": \"Why is the sky blue?\",\n",
        "        \"answer\": \"It's nature's way of preventing eye strain.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative and funny responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the few_shot_prompt_template\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
        "\n",
        "# Run the LLMChain with input_data\n",
        "input_data = {\"query\": \"How can I learn quantum computing?\"}\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b0qYx377mGlR"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How do you feel today?\",\n",
        "        \"answer\": \"As an AI, I don't have feelings, but I've got jokes!\"\n",
        "    }, {\n",
        "        \"query\": \"What is the speed of light?\",\n",
        "        \"answer\": \"Fast enough to make a round trip around Earth 7.5 times in one second!\"\n",
        "    }, {\n",
        "        \"query\": \"What is a quantum computer?\",\n",
        "        \"answer\": \"A magical box that harnesses the power of subatomic particles to solve complex problems.\"\n",
        "    }, {\n",
        "        \"query\": \"Who invented the telephone?\",\n",
        "        \"answer\": \"Alexander Graham Bell, the original 'ringmaster'.\"\n",
        "    }, {\n",
        "        \"query\": \"What programming language is best for AI development?\",\n",
        "        \"answer\": \"Python, because it's the only snake that won't bite.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the capital of France?\",\n",
        "        \"answer\": \"Paris, the city of love and baguettes.\"\n",
        "    }, {\n",
        "        \"query\": \"What is photosynthesis?\",\n",
        "        \"answer\": \"A plant's way of saying 'I'll turn this sunlight into food. You're welcome, Earth.'\"\n",
        "    }, {\n",
        "        \"query\": \"What is the tallest mountain on Earth?\",\n",
        "        \"answer\": \"Mount Everest, Earth's most impressive bump.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the most abundant element in the universe?\",\n",
        "        \"answer\": \"Hydrogen, the basic building block of cosmic smoothies.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the largest mammal on Earth?\",\n",
        "        \"answer\": \"The blue whale, the original heavyweight champion of the world.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the fastest land animal?\",\n",
        "        \"answer\": \"The cheetah, the ultimate sprinter of the animal kingdom.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the square root of 144?\",\n",
        "        \"answer\": \"12, the number of eggs you need for a really big omelette.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the average temperature on Mars?\",\n",
        "        \"answer\": \"Cold enough to make a Martian wish for a sweater and a hot cocoa.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kdui3d7Amxgr"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lyiUSvXHmye2"
      },
      "outputs": [],
      "source": [
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT34c4BkmzqD",
        "outputId": "3f17f8d8-9249-47b5-a193-f8b9ec9315c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alexander Graham Bell, the man who made it possible for us to have awkward phone conversations with our relatives.\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain, FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0)\n",
        "\n",
        "# Existing example and prompt definitions, and dynamic_prompt_template initialization\n",
        "\n",
        "# Create the LLMChain for the dynamic_prompt_template\n",
        "chain = LLMChain(llm=llm, prompt=dynamic_prompt_template)\n",
        "\n",
        "# Run the LLMChain with input_data\n",
        "input_data = {\"query\": \"Who invented the telephone?\"}\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut0T3AsBm4lr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
