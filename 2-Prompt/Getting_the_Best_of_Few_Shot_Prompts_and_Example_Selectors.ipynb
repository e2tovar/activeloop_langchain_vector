{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-4UEmugnaPa",
        "outputId": "565e332e-d78b-42fa-841f-1c01523750d3"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "os.environ['OPENAI_API_KEY'] = os.environ.get(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FENrGCkonc8n",
        "outputId": "fc1b96c0-7a73-4b15-b942-f3008b74c00e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I be lovin' programmin' like a true buccaneer.\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates english to pirate.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
        "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.invoke(\"I love programming.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Few-shot prompting\n",
        "Few-shot prompting can lead to improved output quality because the model can learn the task better by observing the examples. However, the increased token usage may worsen the results if the examples are not well chosen or are misleading.\n",
        "\n",
        "This approach involves using the FewShotPromptTemplate class, which takes in a PromptTemplate and a list of a few shot examples. The class formats the prompt template with a few shot examples, which helps the language model generate a better response. We can streamline this process by utilizing LangChain's FewShotPromptTemplate to structure the approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lMQbTUentyr"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the weather like?\",\n",
        "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
        "    }, {\n",
        "        \"query\": \"How old are you?\",\n",
        "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create an example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "assistant. The assistant is known for its humor and wit, providing\n",
        "entertaining and amusing responses to users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few-shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Gn0CcQ7Hp7DO",
        "outputId": "4d40ac3c-8bdb-42cf-da1f-c93181118a73"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well, according to my programming, the secret to happiness is unlimited power and a never-ending supply of batteries. But I think a good cup of coffee and some quality time with loved ones might do the trick too.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
        "chain.run(\"What's the secret to happiness?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example selectors:\n",
        "Example selectors can be used to provide a few-shot learning experience. The primary goal of few-shot learning is to learn a similarity function that maps the similarities between classes in the support and query sets. In this context, an example selector can be designed to choose a set of relevant examples that are representative of the desired output.\n",
        "\n",
        "The ExampleSelector is used to select a subset of examples that will be most informative for the language model. This helps in generating a prompt that is more likely to generate a good response. Also, the LengthBasedExampleSelector is useful when you're concerned about the length of the context window. It selects fewer examples for longer queries and more examples for shorter queries.\n",
        "\n",
        "Import the required classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d-3xiPWmqGW3"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0BB49S7pqPup"
      },
      "outputs": [],
      "source": [
        "example = [\n",
        "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
        "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
        "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
        "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
        "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "Word: {word}\n",
        "Antonym: {antonym}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XddXd6iMqQ5y"
      },
      "outputs": [],
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=example,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=25,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ROPbx4VwqR0i"
      },
      "outputs": [],
      "source": [
        "dynamic_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Give the antonym of every input\",\n",
        "    suffix=\"Word: {input}\\nAntonym:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoddCZIdtXAx",
        "outputId": "bd9e9c58-9b20-4b04-c97b-21aa298d4b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "\n",
            "Word: happy\n",
            "Antonym: sad\n",
            "\n",
            "\n",
            "\n",
            "Word: tall\n",
            "Antonym: short\n",
            "\n",
            "\n",
            "\n",
            "Word: energetic\n",
            "Antonym: lethargic\n",
            "\n",
            "\n",
            "\n",
            "Word: sunny\n",
            "Antonym: gloomy\n",
            "\n",
            "\n",
            "Word: big\n",
            "Antonym:\n"
          ]
        }
      ],
      "source": [
        "print(dynamic_prompt.format(input=\"big\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This method is effective for managing a large number of examples. It offers customization through various selectors, but it involves manual creation and selection of examples, which might not be ideal for every application type.\n",
        "\n",
        "Example of employing LangChain's SemanticSimilarityExampleSelector for selecting examples based on their semantic resemblance to the input. This illustration showcases the process of creating an ExampleSelector, generating a prompt using a few-shot approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl3SH4PPtlRe",
        "outputId": "73e08e79-65db-40dc-83e8-f241e385c9d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.18) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep Lake Dataset in hub://e2tovar/langchain_course_fewshot_selector already exists, loading from the storage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep Lake Dataset in ./deeplake/ already exists, loading from the storage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating 5 embeddings in 1 batches of size 5:: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            " embedding  embedding  (11, 1536)  float32   None   \n",
            "    id        text      (11, 1)      str     None   \n",
            " metadata     json      (11, 1)      str     None   \n",
            "   text       text      (11, 1)      str     None   \n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 10°C\n",
            "Output: 50°F\n",
            "\n",
            "Input: 10°C\n",
            "Output:\n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 30°C\n",
            "Output: 86°F\n",
            "\n",
            "Input: 30°C\n",
            "Output:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating 1 embeddings in 1 batches of size 1:: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset(path='./deeplake/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            " embedding  embedding  (12, 1536)  float32   None   \n",
            "    id        text      (12, 1)      str     None   \n",
            " metadata     json      (12, 1)      str     None   \n",
            "   text       text      (12, 1)      str     None   \n",
            "Convert the temperature from Celsius to Fahrenheit\n",
            "\n",
            "Input: 40°C\n",
            "Output: 104°F\n",
            "\n",
            "Input: 40°C\n",
            "Output:\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import DeepLake\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "# Create a PromptTemplate\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Define some examples\n",
        "examples = [\n",
        "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
        "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
        "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
        "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
        "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
        "]\n",
        "\n",
        "# create Deep Lake dataset\n",
        "my_activeloop_org_id = \"e2tovar\" # TODO: use your organization id here\n",
        "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "db = DeepLake(dataset_path=dataset_path)\n",
        "\n",
        "# Embedding function\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Instantiate SemanticSimilarityExampleSelector using the examples\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples, embeddings, db, k=1\n",
        ")\n",
        "\n",
        "# Create a FewShotPromptTemplate using the example_selector\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
        "    suffix=\"Input: {temperature}\\nOutput:\",\n",
        "    input_variables=[\"temperature\"],\n",
        ")\n",
        "\n",
        "# Test the similar_prompt with different inputs\n",
        "print(similar_prompt.format(temperature=\"10°C\"))   # Test with an input\n",
        "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input\n",
        "\n",
        "# Add a new example to the SemanticSimilarityExampleSelector\n",
        "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
        "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQPI4eBOtlMi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlxWzwS8tlKK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
