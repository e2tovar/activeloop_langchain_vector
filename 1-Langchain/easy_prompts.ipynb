{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_key = os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hf_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\n__root__\n  Got invalid task fill-mask, currently only dict_keys(['translation', 'summarization', 'conversational', 'text-generation', 'text2text-generation']) are supported (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\1-Langchain\\easy_prompts.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m \u001b[39mimport\u001b[39;00m HuggingFaceHub, LLMChain\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# initialize Hub LLM\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m hub_llm \u001b[39m=\u001b[39m HuggingFaceHub(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         repo_id\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdistilbert/distilbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m0\u001b[39;49m}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# create prompt template > LLM chain\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     prompt\u001b[39m=\u001b[39mprompt,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     llm\u001b[39m=\u001b[39mhub_llm\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eddy.Tovar/Documents/Datos/work/PycharmProjects/proyectos_personales/cursos_llm/activeloop_langchain_vector/1-Langchain/easy_prompts.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\langchain_core\\load\\serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\Eddy.Tovar\\Documents\\Datos\\work\\PycharmProjects\\proyectos_personales\\cursos_llm\\activeloop_langchain_vector\\venv\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\n__root__\n  Got invalid task fill-mask, currently only dict_keys(['translation', 'summarization', 'conversational', 'text-generation', 'text2text-generation']) are supported (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris')], [Generation(text='giraffe')], [Generation(text='nitrogen')], [Generation(text='yellow')]] llm_output=None run=[RunInfo(run_id=UUID('9c30e640-b91d-420c-ac2d-bb1ea280db71')), RunInfo(run_id=UUID('444573c4-9232-460c-96f3-eb3f2ca67471')), RunInfo(run_id=UUID('0223a431-82ed-42c4-8166-2f64039a4e5f')), RunInfo(run_id=UUID('ff6b1ea6-3728-4fdd-97d3-16317451e129'))]\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify our prompt template to include multiple questions to implement a second approach. The language model will understand that we have multiple questions and answer them sequentially. This method performs best on more capable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. The capital city of France is Paris.\n",
      "2. The largest mammal on Earth is the blue whale.\n",
      "3. The gas most abundant in Earth's atmosphere is nitrogen.\n",
      "4. A ripe banana is typically yellow.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "opai_llm = OpenAI(name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=opai_llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "ans = llm_chain.run(qs_str)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a prompt template for summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"LangChain provides many modules that can be used to build language model applications.\n",
    "Modules can be combined to create more complex applications, or be used individually for simple applications.\n",
    "The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example\n",
    "of how to do this. For this purpose, let’s pretend we are building\n",
    "a service that generates a company name based on what the company makes.\"\"\"\n",
    "\n",
    "summarized_text = summarization_chain.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLangChain offers various modules for building language model applications, which can be used together or separately, with the most basic being calling an LLM on input, as demonstrated in a simple example of creating a company name based on its product.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mi perro está loco.\n"
     ]
    }
   ],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"Spanish\"\n",
    "text = \"My dog is crazy\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
